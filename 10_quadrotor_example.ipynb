{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import casadi as cas\n",
    "from datetime import datetime\n",
    "import auxiliaries as aux\n",
    "import polytope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 20  # Set the prediction horizon.\n",
    "disc_step_size = 0.05  # Set step size for discretization.\n",
    "last_simulation_time = 10  # Set the simulation time in seconds.\n",
    "\n",
    "coop_weight = 12  # Set a weight for the cooperation offset cost.\n",
    "\n",
    "last_simulation_time = int(last_simulation_time/disc_step_size)  # Convert the simulation time into seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Horizon is', horizon*disc_step_size, 'seconds.')\n",
    "print('Simulation time is', last_simulation_time, 'which corresponds to', last_simulation_time*disc_step_size, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the agents.\"\"\"\n",
    "num_agents = 3  # Set the number of agents.\n",
    "\n",
    "agents = []  # Initialize a list to collect the agents in.\n",
    "for i in range(num_agents):\n",
    "    # Initialize the quadrotor agent.\n",
    "    agents.append(aux.Quadrotor(disc_step_size))\n",
    "\n",
    "# Specify the shift in the altitude.\n",
    "altitude_shifts = [0.0, -2.0, -4.0]\n",
    "\n",
    "# Specifiy initial states.\n",
    "initial_state_list = [\n",
    "  np.array([[1.0], [1.0], [0.], [0], [0], [0], [0], [0], [0], [0]]),\n",
    "  np.array([[4.0], [3.0], [2.3], [0], [0], [0], [0], [0], [0], [0]]),\n",
    "  np.array([[7.0], [3.0], [3.9], [0], [0], [0], [0], [0], [0], [0]])\n",
    "  ]\n",
    "\n",
    "for i, agent in enumerate(agents):\n",
    "    x = cas.MX.sym('x', 10)\n",
    "    u = cas.MX.sym('u', 3)\n",
    "    output_map = cas.Function(\n",
    "      'output', \n",
    "      [x, u], \n",
    "      [cas.vertcat(x[0], x[1], x[2] + altitude_shifts[i])], \n",
    "      ['x', 'u'], \n",
    "      ['y']) \n",
    "    agent.output_map = output_map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out polytopes for the position on the $x$-$y$-plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_constraint_vertices = []  # Save the vertices of the state constraints in this list.\n",
    "state_constraint_polytopes = []  # Save the polytopes in this list to access their halfplane representation.\n",
    "cooperation_constraint_vertices = []  # Save the vertices of the cooperation outputs' constraints in this list.\n",
    "cooperation_constraint_polytopes = []  # Save the polytopes in this list to access their halfplane representation.\n",
    "\n",
    "r_feasible_list = []  # Provide some admissible references on the boundary of their constraint sets.\n",
    "\n",
    "GRAV = agents[0].g\n",
    "KT = agents[0].k_thrust\n",
    "\n",
    "# Parameters of the first polytope:\n",
    "b1 = 2\n",
    "b2 = 3\n",
    "h1 = 1.5\n",
    "h2 = 2\n",
    "sp = [0.0, 0.0]  # Start point (lower left corner).\n",
    "vertices = np.array([[sp[0]             , sp[1]],\n",
    "                     [sp[0] + b1 + b2   , sp[1]],\n",
    "                     [sp[0] + b1 + b2   , sp[1] + h1],\n",
    "                     [sp[0] + b1        , sp[1] + h1 + h2],\n",
    "                     [sp[0]             , sp[1] + h1 + h2]])\n",
    "state_constraint_vertices.append(vertices)\n",
    "state_constraint_polytopes.append(polytope.qhull(vertices))\n",
    "sp_offset = [0.1, 0.1]\n",
    "mult_offset = 0.9\n",
    "vertices = np.array([[sp[0] + sp_offset[0]            , sp[1] + sp_offset[1]],\n",
    "                     [sp[0] - sp_offset[0] + b1 + b2   , sp[1] + sp_offset[1]],\n",
    "                     [sp[0] - sp_offset[0] + b1 + b2   , sp[1] - sp_offset[1] + h1],\n",
    "                     [sp[0] - sp_offset[0] + b1        , sp[1] - sp_offset[1] + h1 + h2],\n",
    "                     [sp[0] + sp_offset[0]            , sp[1] - sp_offset[1] + h1 + h2]])\n",
    "cooperation_constraint_vertices.append(vertices)\n",
    "cooperation_constraint_polytopes.append(polytope.qhull(vertices))\n",
    "\n",
    "r_feasible_list.append([(np.array([[0.1, 0.1, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]).T, \n",
    "                         np.array([[0.0, 0.0, GRAV/KT]]).T)])\n",
    "\n",
    "b1 = 1.0\n",
    "h1 = 3.5\n",
    "sp = [3.5, 0.0]  # Start point (lower left corner).\n",
    "vertices = np.array([[sp[0]        , sp[1]],\n",
    "                     [sp[0] + b1   , sp[1]],\n",
    "                     [sp[0] + b1   , sp[1] + h1],\n",
    "                     [sp[0]        , sp[1] + h1]])\n",
    "state_constraint_vertices.append(vertices)\n",
    "state_constraint_polytopes.append(polytope.qhull(vertices))\n",
    "sp_offset = [0.1, 0.1]\n",
    "mult_offset = 0.9\n",
    "vertices = np.array([[sp[0] + sp_offset[0]     , sp[1]+ sp_offset[1]],\n",
    "                     [sp[0] - sp_offset[0] + b1, sp[1] + sp_offset[1]],\n",
    "                     [sp[0] - sp_offset[0] + b1, sp[1] - sp_offset[1] + h1],\n",
    "                     [sp[0] + sp_offset[0]     , sp[1] - sp_offset[1] + h1]])\n",
    "cooperation_constraint_vertices.append(vertices)\n",
    "cooperation_constraint_polytopes.append(polytope.qhull(vertices))\n",
    "\n",
    "r_feasible_list.append([(np.array([[3.1, 0.1, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]).T, \n",
    "                         np.array([[0.0, 0.0, GRAV/KT]]).T)])\n",
    "\n",
    "b1 = 3\n",
    "b2 = 2\n",
    "h1 = 1.5\n",
    "h2 = 2\n",
    "sp = [3.0, 0.0]  # Start point (lower left corner).\n",
    "vertices = np.array([[sp[0]             , sp[1]],\n",
    "                     [sp[0] + b1 + b2   , sp[1]],\n",
    "                     [sp[0] + b1 + b2   , sp[1] + h1 + h2],\n",
    "                     [sp[0] + b1        , sp[1] + h1 + h2],\n",
    "                     [sp[0]             , sp[1] + h1]])\n",
    "state_constraint_vertices.append(vertices)\n",
    "state_constraint_polytopes.append(polytope.qhull(vertices))\n",
    "sp_offset = [0.1, 0.1]\n",
    "mult_offset = 0.9\n",
    "vertices = np.array([[sp[0] + sp_offset[0]            , sp[1] + sp_offset[1]],\n",
    "                     [sp[0] - sp_offset[0] + b1 + b2  , sp[1] + sp_offset[1]],\n",
    "                     [sp[0] - sp_offset[0] + b1 + b2  , sp[1] - sp_offset[1] + h1 + h2],\n",
    "                     [sp[0] + sp_offset[0] + b1       , sp[1] - sp_offset[1] + h1 + h2],\n",
    "                     [sp[0] + sp_offset[0]            , sp[1] - sp_offset[1] + h1]])\n",
    "cooperation_constraint_vertices.append(vertices)\n",
    "cooperation_constraint_polytopes.append(polytope.qhull(vertices))\n",
    "\n",
    "r_feasible_list.append([(np.array([[3.1, 0.1, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]).T, \n",
    "                         np.array([[0.0, 0.0, GRAV/KT]]).T)])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(True, which='both')  # Draw grid lines.\n",
    "ax.axis('equal')  # Make plot a box.\n",
    "\n",
    "for vertices in state_constraint_vertices:\n",
    "    ax.fill(vertices[:,0], vertices[:,1],\n",
    "        edgecolor='black', linewidth=1,\n",
    "        facecolor=(0.5,0.5,0.5,0.5))  # draw polytope\n",
    "for vertices in cooperation_constraint_vertices:\n",
    "    ax.fill(vertices[:,0], vertices[:,1],\n",
    "        edgecolor='orange', linewidth=1,\n",
    "        facecolor=(0.5,0.2,0.2,0.2))  # draw polytope\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set constraints for each agent.\"\"\"\n",
    "for i, agent in enumerate(agents):\n",
    "    ## Set the state constraints.\n",
    "    # Use the defined polytopes for constraints on the first and second position (the planar ones.)\n",
    "    A = np.hstack([state_constraint_polytopes[i].A, np.zeros([state_constraint_polytopes[i].A.shape[0], agent.state_dim - 2])])\n",
    "    b = np.vstack(state_constraint_polytopes[i].b) \n",
    "    # Allow a difference of 1.0 to the shifted base altitude of the agents.\n",
    "    A = np.vstack([A, np.array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, -1, 0, 0, 0, 0, 0, 0, 0]])])\n",
    "    b = np.vstack([b, np.array([[-altitude_shifts[i] + 1.0], [-(-altitude_shifts[i] - 1.0)]])])\n",
    "    \n",
    "    # Bound the remaining states.\n",
    "    A = np.vstack([A, np.array([[0, 0, 0,  1,  0,  0,  0,  0,  0,  0], \n",
    "                                [0, 0, 0, -1,  0,  0,  0,  0,  0,  0],\n",
    "                                [0, 0, 0,  0,  1,  0,  0,  0,  0,  0], \n",
    "                                [0, 0, 0,  0, -1,  0,  0,  0,  0,  0],\n",
    "                                [0, 0, 0,  0,  0,  1,  0,  0,  0,  0], \n",
    "                                [0, 0, 0,  0,  0, -1,  0,  0,  0,  0],\n",
    "                                [0, 0, 0,  0,  0,  0,  1,  0,  0,  0], \n",
    "                                [0, 0, 0,  0,  0,  0, -1,  0,  0,  0],\n",
    "                                [0, 0, 0,  0,  0,  0,  0,  1,  0,  0], \n",
    "                                [0, 0, 0,  0,  0,  0,  0, -1,  0,  0],\n",
    "                                [0, 0, 0,  0,  0,  0,  0,  0,  1,  0], \n",
    "                                [0, 0, 0,  0,  0,  0,  0,  0, -1,  0],\n",
    "                                [0, 0, 0,  0,  0,  0,  0,  0,  0,  1], \n",
    "                                [0, 0, 0,  0,  0,  0,  0,  0,  0, -1]])])\n",
    "    b = np.vstack([\n",
    "        b, \n",
    "        np.array([\n",
    "            [np.pi/4], [np.pi/4], # theta; pitch angle\n",
    "            [np.pi/4], [np.pi/4], # phi; roll angle\n",
    "            [2.0], [2.0], # v1\n",
    "            [2.0], [2.0], # v2\n",
    "            [2.0], [2.0], # v3\n",
    "            [3.0], [3.0], # omega_theta\n",
    "            [3.0], [3.0]  # omega_phi\n",
    "        ])])\n",
    "    agent.state_constraints['A'] = A\n",
    "    agent.state_constraints['b'] = b\n",
    "\n",
    "    # Set input constraints.\n",
    "    agent.input_constraints['A'] = np.array([\n",
    "        [ 1,  0,  0], # u1\n",
    "        [-1,  0,  0], # u1\n",
    "        [ 0,  1,  0], # u2\n",
    "        [ 0, -1,  0], # u2\n",
    "        [ 0,  0,  1], # u3\n",
    "        [ 0,  0, -1]  # u3\n",
    "        ])\n",
    "    agent.input_constraints['b'] = np.array([\n",
    "        [np.pi/9],  [np.pi/9],      # u1\n",
    "        [np.pi/9],  [np.pi/9],      # u2\n",
    "        [2*agent.g],   [0]])     # u3 (thrust)\n",
    "    \n",
    "    # Set constraints on the cooperation input reference.\n",
    "    agent.cooperation_constraints['Au'] = agent.input_constraints['A']\n",
    "    agent.cooperation_constraints['bu'] = np.array([\n",
    "            [0.0], [0.0],  \n",
    "            [0.0], [0.0],\n",
    "            [19.5], [0.05]\n",
    "        ])\n",
    "\n",
    "    ## Set the state constraints.\n",
    "    # Use the defined polytopes for constraints on the first and second position (the planar ones.)\n",
    "    Ay = np.hstack([cooperation_constraint_polytopes[i].A, np.zeros([cooperation_constraint_polytopes[i].A.shape[0], 1])])\n",
    "    by = np.vstack(cooperation_constraint_polytopes[i].b)\n",
    "    agent.cooperation_constraints['Ay'] = np.vstack([Ay, np.array([[0, 0, 1], [0, 0, -1]])])\n",
    "    agent.cooperation_constraints['by'] = np.vstack([by, np.array([[0.9], [0.9]])])\n",
    "    \n",
    "    Ax = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
    "    agent.cooperation_constraints['Ax'] = np.vstack(\n",
    "        [Ay@Ax, np.array([\n",
    "            [0, 0,  1,  0,  0,  0,  0,  0,  0,  0], \n",
    "            [0, 0, -1,  0,  0,  0,  0,  0,  0,  0],\n",
    "            [0, 0,  0,  1,  0,  0,  0,  0,  0,  0], \n",
    "            [0, 0,  0, -1,  0,  0,  0,  0,  0,  0],\n",
    "            [0, 0,  0,  0,  1,  0,  0,  0,  0,  0], \n",
    "            [0, 0,  0,  0, -1,  0,  0,  0,  0,  0],\n",
    "            [0, 0,  0,  0,  0,  1,  0,  0,  0,  0], \n",
    "            [0, 0,  0,  0,  0, -1,  0,  0,  0,  0],\n",
    "            [0, 0,  0,  0,  0,  0,  1,  0,  0,  0], \n",
    "            [0, 0,  0,  0,  0,  0, -1,  0,  0,  0],\n",
    "            [0, 0,  0,  0,  0,  0,  0,  1,  0,  0], \n",
    "            [0, 0,  0,  0,  0,  0,  0, -1,  0,  0],\n",
    "            [0, 0,  0,  0,  0,  0,  0,  0,  1,  0], \n",
    "            [0, 0,  0,  0,  0,  0,  0,  0, -1,  0],\n",
    "            [0, 0,  0,  0,  0,  0,  0,  0,  0,  1], \n",
    "            [0, 0,  0,  0,  0,  0,  0,  0,  0, -1]])])\n",
    "    agent.cooperation_constraints['bx'] = np.vstack([\n",
    "        by, \n",
    "        np.array([\n",
    "            [-altitude_shifts[i] + 0.9], [-(-altitude_shifts[i] - 0.9)], # altitude\n",
    "            [0.], [0.], # theta; pitch angle\n",
    "            [0.], [0.], # phi; roll angle\n",
    "            [0.], [0.], # v1\n",
    "            [0.], [0.], # v2\n",
    "            [0.], [0.], # v3\n",
    "            [0.], [0.], # omega_theta\n",
    "            [0.], [0.]  # omega_phi\n",
    "        ])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Neighbours\"\"\"\n",
    "agents[0].neighbours = [agents[2]]\n",
    "agents[1].neighbours = [agents[2]]\n",
    "agents[2].neighbours = [agents[0], agents[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the stage costs.\"\"\"\n",
    "for agent in agents:\n",
    "    # Define the artificial equilibrium. \n",
    "    x = cas.MX.sym('x', agent.state_dim)\n",
    "    u = cas.MX.sym('u', agent.input_dim)\n",
    "    xT = cas.MX.sym('xT', agent.state_dim)\n",
    "    uT = cas.MX.sym('uT', agent.input_dim)\n",
    "    \n",
    "    # Set the weight for the distance of the state to the equilibrium.\n",
    "    Q = np.diag([\n",
    "        1., 1., 1.,                             # z1, z2, z3\n",
    "        0.1/(np.pi/4)**2, 0.1/(np.pi/4)**2,     # theta, phi\n",
    "        0.01/4, 0.01/4, 1/4,                 # v1, v2, v3\n",
    "        0.01/3**2, 0.01/3**2                    # omega_theta, omega_phi\n",
    "    ])\n",
    "            \n",
    "    # Set the weight for the distance of the input to the equilibrium.\n",
    "    R = np.diag([\n",
    "        0.001/np.pi/9**2, \n",
    "        0.001/np.pi/9**2, \n",
    "        0.001/(2*agent.g)**2\n",
    "        ])\n",
    "        \n",
    "    # Add stage cost to agents.\n",
    "    agent.stage_cost = cas.Function(\n",
    "        'stage_cost', \n",
    "        [x, u, xT, uT], \n",
    "        [ (x - xT).T@Q@(x - xT) + (u - uT).T@R@(u - uT) ],\n",
    "        ['x', 'u', 'xT', 'uT'], \n",
    "        ['l'])\n",
    "    agent.stage_cost_weights = {'Q': Q, 'R': R}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cooperation offset cost\"\"\"\n",
    "yc1 = cas.MX.sym('yc1', 3)\n",
    "yc2 = cas.MX.sym('yc2', 3)\n",
    "\n",
    "# Consensus on all outputs.\n",
    "bilat_coop_cost = cas.Function('cooperation_cost', [yc1, yc2], [ coop_weight*(yc1 - yc2).T@(yc1 - yc2) ], ['yc1', 'yc2'], ['V_ij^c'])\n",
    "\n",
    "# Set the bilateral cooperation cost for each agent.\n",
    "for agent in agents:\n",
    "    agent.bilat_coop_cost = bilat_coop_cost\n",
    "    \n",
    "del yc1, yc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Terminal ingredients\"\"\"\n",
    "aux.compute_terminal_ingredients_for_quadrotor(\n",
    "    agent=agents[0],\n",
    "    grid_resolution=1, \n",
    "    num_decrease_samples=1000, \n",
    "    alpha = 0.03,\n",
    "    alpha_tol = 1e-9,\n",
    "    references_are_equilibria=True,\n",
    "    compute_size_for_decrease=False,\n",
    "    compute_size_for_constraints=True,\n",
    "    epsilon=1e-2,\n",
    "    verbose=2,\n",
    "    solver='MOSEK')\n",
    "for agent in agents[1:]:\n",
    "    agent.terminal_ingredients = agents[0].terminal_ingredients\n",
    "    agent.terminal_ingredients['type'] = 'generalized'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation of the closed loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Apply the MPC algorithm.\"\"\"\n",
    "# Keep track of the closed-loop system.\n",
    "closed_loop_evolution = []\n",
    "\n",
    "for i, agent in enumerate(agents):\n",
    "    # Initialize the current state of the agent.\n",
    "    agent.current_state = initial_state_list[i].copy()\n",
    "    # Save the closed-loop state evolution of each agent as an attribute of the agent.\n",
    "    agent.cl_x = [agent.current_state.copy()]\n",
    "    # Take output given by initial state as first cooperation output.\n",
    "    agent.current_cooperation_output = np.copy(agent.current_state[0:3])\n",
    "\n",
    "for t in range(last_simulation_time+1):\n",
    "    if t % 10 == 0:\n",
    "        print(f'Current time: {t*disc_step_size} seconds at simulation step {t}.')\n",
    "    # Track the time.\n",
    "    closed_loop_evolution.append([None]*len(agents))\n",
    "    \n",
    "    # Go in sequence over the agents.\n",
    "    for agent_index, agent in enumerate(agents):\n",
    "        closed_loop_evolution[t][agent_index] = {\"time\":t}\n",
    "        agent.current_time = t\n",
    "        # Keep track of the current state.\n",
    "        closed_loop_evolution[t][agent_index].update({\"current_state\":np.copy(agent.current_state)})\n",
    "        \n",
    "        # Generate a warm start. Decision vector: (u, x, uc, xc, yc)\n",
    "        if t == 0:\n",
    "            #warm_start = np.zeros((u.shape[0] + x.shape[0] + uc.shape[0] + xc.shape[0] + yc.shape[0], 1))\n",
    "            warm_start = []\n",
    "            # Add warm start of the input trajectory.\n",
    "            for i in range(horizon):\n",
    "                warm_start.append(np.array([[0], [0], [9.81/0.91]]))\n",
    "            # Add warm start of the state trajectory.\n",
    "            for i in range(horizon):\n",
    "                warm_start.append(agent.current_state)\n",
    "            # Add warm start of the cooperation input.\n",
    "            warm_start.append(np.array([[0], [0], [9.81/0.91]]))\n",
    "            # Add warm start of the cooperation state.\n",
    "            warm_start.append(agent.current_state)\n",
    "            # Add warm start of the cooperation output.\n",
    "            warm_start.append(agent.current_state[0:3])\n",
    "            warm_start = np.concatenate(warm_start)\n",
    "        else:\n",
    "            if agent.terminal_ingredients['type'] == 'equality':\n",
    "                warm_start = []\n",
    "                # Append warm start of the input trajectoy by taking the old one, shifting it and appending the currently optimal equilibrium's input.\n",
    "                for i in range(horizon-1):\n",
    "                    warm_start.append(agent.current_MPC_sol[\"u_opt\"][0:agent.input_dim, i+1:i+2])\n",
    "                warm_start.append(agent.current_MPC_sol[\"uc_opt\"])\n",
    "                # Append warm start of the state trajectory by taking the old one, shifting it and appending the currently optimal equilibrium's state.\n",
    "                for i in range(horizon-1):\n",
    "                    warm_start.append(agent.current_MPC_sol[\"x_opt\"][0:agent.state_dim, i+1:i+2])\n",
    "                warm_start.append(agent.current_MPC_sol[\"xc_opt\"])\n",
    "\n",
    "                # Append warm start of cooperation input.\n",
    "                warm_start.append(agent.current_MPC_sol[\"uc_opt\"])\n",
    "                # Append warm start of the cooperation state.\n",
    "                warm_start.append(agent.current_MPC_sol[\"xc_opt\"])\n",
    "                \n",
    "                # Append the warm start of the cooperation output.\n",
    "                warm_start.append(agent.current_MPC_sol[\"yc_opt\"])\n",
    "                \n",
    "                warm_start = np.concatenate(warm_start)\n",
    "            else:\n",
    "                warm_start = []\n",
    "                # Create the shifted warm start.\n",
    "                for i in range(horizon-1):\n",
    "                    warm_start.append(agent.current_MPC_sol[\"u_opt\"][0:agent.input_dim, i+1:i+2])\n",
    "                X = agent.terminal_ingredients['X']\n",
    "                Y = agent.terminal_ingredients['Y']\n",
    "                P = X['static'].copy()\n",
    "                K = Y['static'].copy()\n",
    "                P = np.linalg.inv(P)\n",
    "                # Ensure that P is symmetric.\n",
    "                P = 0.5*(P + P.T)\n",
    "                K_terminal_opt = K@P  # Compute the terminal control matrix.\n",
    "                \n",
    "                Kf = agent.current_MPC_sol[\"uc_opt\"] + K_terminal_opt@(agent.current_MPC_sol[\"x_opt\"][0:agent.state_dim, -1:] - agent.current_MPC_sol[\"xc_opt\"])\n",
    "                warm_start.append(Kf)\n",
    "                # Append warm start of the state trajectory by taking the old one, shifting it and appending the currently optimal equilibrium's state.\n",
    "                for i in range(horizon-1):\n",
    "                    warm_start.append(agent.current_MPC_sol[\"x_opt\"][0:agent.state_dim, i+1:i+2])\n",
    "                warm_start.append(agent.dynamics(agent.current_MPC_sol[\"x_opt\"][0:agent.state_dim, -1:], Kf))\n",
    "\n",
    "                # Append warm start of cooperation input.\n",
    "                warm_start.append(agent.current_MPC_sol[\"uc_opt\"])\n",
    "                # Append warm start of the cooperation state.\n",
    "                warm_start.append(agent.current_MPC_sol[\"xc_opt\"])\n",
    "                \n",
    "                # Append the warm start of the cooperation output, build that from the available data of the neighbours.\n",
    "                yc_ws = agent.current_MPC_sol[\"yc_opt\"]\n",
    "                for neighbour in agent.neighbours:\n",
    "                    yc_ws += 1e-3*(neighbour.current_MPC_sol[\"yc_opt\"] - agent.current_MPC_sol[\"yc_opt\"])\n",
    "                warm_start.append(yc_ws)\n",
    "                \n",
    "                warm_start = np.concatenate(warm_start)\n",
    "        \n",
    "        # Solve the MPC problem.\n",
    "        agent.current_MPC_sol = aux.MPC_for_cooperation(agent, horizon=horizon, warm_start=warm_start, terminal_ingredients=agent.terminal_ingredients)\n",
    "        # Keep track of the solution.\n",
    "        closed_loop_evolution[t][agent_index].update(agent.current_MPC_sol)\n",
    "        \n",
    "        # Update the current state of the agent. \n",
    "        #agent.current_state = np.copy(agent.current_MPC_sol[\"x_opt\"][0:agent.state_dim, 1:2])\n",
    "        agent.current_state = np.copy(agent.dynamics(agent.current_state, agent.current_MPC_sol[\"u_opt\"][0:agent.input_dim, 0:1]))\n",
    "        agent.cl_x.append(np.array(agent.current_state))\n",
    "                \n",
    "        # Update the cooperation output of the agent.\n",
    "        agent.current_cooperation_output = np.copy(agent.current_MPC_sol[\"yc_opt\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "# Plots\n",
    "#### Plot the closed-loop evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in agents:\n",
    "    if type(agent.cl_x) == list:\n",
    "        agent.cl_x = np.hstack(agent.cl_x)\n",
    "        \n",
    "colours = [\n",
    "    \"#0072B2\",  # blue\n",
    "    \"#D55E00\",  # orange\n",
    "    \"#009E73\",  # green\n",
    "    \"#CC79A7\",  # magenta\n",
    "    \"#56B4E9\",  # light blue\n",
    "    \"#E69F00\",  # yellow-orange\n",
    "    \"#B22222\",  # red\n",
    "    \"#6A3D9A\",  # purple\n",
    "    \"#117733\",  # teal green\n",
    "    \"#88CCEE\",  # cyan\n",
    "    \"#DDCC77\",  # muted yellow-orange\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the closed-loop evolution.\n",
    "time_steps = range(0, last_simulation_time+1)\n",
    "\n",
    "# Create a vector with time in seconds.\n",
    "time = []\n",
    "for t in time_steps:\n",
    "    time.append(t*disc_step_size)\n",
    "    \n",
    "fig1_states, (ax_states_1, ax_states_2, ax_states_3) = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(20,6))\n",
    "fig2_states, (ax_states_4, ax_states_5, ax_states_6) = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(20,6))\n",
    "fig3_states, (ax_states_7, ax_states_8, ax_states_9, ax_states_10) = plt.subplots(nrows=1, ncols=4, sharex=True, figsize=(20,6))\n",
    "fig_inputs, (ax_u1, ax_u2, ax_u3) = plt.subplots(nrows=1, ncols=3, sharex=True, figsize=(20,6))\n",
    "\n",
    "for ax_i in fig1_states.axes:\n",
    "    ax_i.grid(True)\n",
    "for ax_i in fig2_states.axes:\n",
    "    ax_i.grid(True)\n",
    "for ax_i in fig3_states.axes:\n",
    "    ax_i.grid(True)\n",
    "for ax_i in fig_inputs.axes:\n",
    "    ax_i.grid(True)\n",
    "    \n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.grid(True)\n",
    "ax3 = plt.figure().add_subplot(projection='3d')\n",
    "ax3.grid(True)\n",
    "\n",
    "end_states = []  # Save the state at the end of the simulation.\n",
    "for agent in agents:\n",
    "    # Extract state evolution of agent.\n",
    "    agent_state_evo = []\n",
    "    for t in time_steps:\n",
    "        agent_state_evo.append(closed_loop_evolution[t][agents.index(agent)][\"current_state\"])\n",
    "    # Build state evolution matrix.\n",
    "    state_evo_mat = np.concatenate(agent_state_evo, axis=1)\n",
    "    \n",
    "    # Add the state at the end of the simulation.\n",
    "    end_states.append(state_evo_mat[:, -1:])\n",
    "    \n",
    "    # Extract input evolution of agent.\n",
    "    agent_input_evo = []\n",
    "    for t in time_steps:\n",
    "        agent_input_evo.append(closed_loop_evolution[t][agents.index(agent)][\"u_opt\"][0:agent.input_dim, 0:1])\n",
    "    input_evo_mat = np.concatenate(agent_input_evo, axis=1)\n",
    "    \n",
    "    label_str = \"Agent \" + str(agent.id)  # For labelling the plot.\n",
    "    \n",
    "    ax_states_1.plot(time, state_evo_mat[0, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_1.set_ylabel('pos1')\n",
    "    ax_states_1.set_xlabel('time in s')\n",
    "    \n",
    "    ax_states_2.plot(time, state_evo_mat[1, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_2.set_ylabel('pos2')\n",
    "    ax_states_2.set_xlabel('time in s')\n",
    "    \n",
    "    # Plot evolution of the altitute.\n",
    "    ax_states_3.plot(time, state_evo_mat[2, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_3.set_ylabel('altitute in m')\n",
    "    ax_states_3.set_xlabel('time in s')\n",
    "    \n",
    "    ax_states_4.plot(time, np.degrees(state_evo_mat[3, :]), label=label_str, marker='x', markersize=0)\n",
    "    ax_states_4.set_ylabel('x_4')\n",
    "    ax_states_4.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_5.plot(time, np.degrees(state_evo_mat[4, :]), label=label_str, marker='x', markersize=0)\n",
    "    ax_states_5.set_ylabel('x_5')\n",
    "    ax_states_5.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_6.plot(time, state_evo_mat[5, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_6.set_ylabel('x_6')\n",
    "    ax_states_6.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_7.plot(time, state_evo_mat[6, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_7.set_ylabel('x_7')\n",
    "    ax_states_7.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_8.plot(time, state_evo_mat[7, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_8.set_ylabel('x_8')\n",
    "    ax_states_8.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_9.plot(time, state_evo_mat[8, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_9.set_ylabel('x_9')\n",
    "    ax_states_9.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_states_10.plot(time, state_evo_mat[9, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_states_10.set_ylabel('x_10')\n",
    "    ax_states_10.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_u1.plot(time, input_evo_mat[0, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_u1.set_ylabel('u_1')\n",
    "    ax_u1.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_u2.plot(time, input_evo_mat[1, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_u2.set_ylabel('u_2')\n",
    "    ax_u2.set_xlabel('time in seconds')\n",
    "    \n",
    "    ax_u3.plot(time, input_evo_mat[2, :], label=label_str, marker='x', markersize=0)\n",
    "    ax_u3.set_ylabel('u_3')\n",
    "    ax_u3.set_xlabel('time in seconds')\n",
    "        \n",
    "    # Plot 2D evolution of position without altitute.\n",
    "    ax2.plot(state_evo_mat[0, :], state_evo_mat[1, :], label=label_str, marker='x', markersize=0)\n",
    "    \n",
    "    # Plot 3D evolution of position.\n",
    "    ax3.plot(state_evo_mat[0, :], state_evo_mat[1, :], state_evo_mat[2, :])\n",
    "\n",
    "for vertices in state_constraint_vertices:\n",
    "    ax2.fill(vertices[:,0], vertices[:,1],\n",
    "             edgecolor='black', linewidth=1,\n",
    "             facecolor=(0.05,0.05,0.05,0.1))  # draw polytope\n",
    "    \n",
    "# ax2.set_xlim([4.0,4.5])\n",
    "# ax2.set_ylim([1.5, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
